import subprocess
import sys
import time
import os
import json
from pathlib import Path
from datetime import datetime
import numpy as np
from master_bucket_utils import MasterBucketDatabase
from data_utils import get_bucket_description
import pandas as pd
import plotly.graph_objects as go

CURRENT_SEASON = "2025-26"

BASE_DIR = Path(__file__).resolve().parent

UNIQORN_LEADERS_MASTER_FILE = "Uniqorn_Master.xlsx"
MOST_RECENT_GAMES_MASTER_FILE = "Most_Recent_Games_Master.xlsx"
CURRENT_SEASON_UNIQORN_GAMES_MASTER_FILE = "CurrentSeason_UniqornGames_Master.xlsx"
ULTIMATE_UNIQORN_GAMES_MASTER_FILE = "Ultimate_Uniqorn_Games_Master.xlsx"
PREVIOUS_ULTIMATE_UNIQORN_GAMES_MASTER_FILE = "Previous_Ultimate_Uniqorn_Games_Master.xlsx"
ULTIMATE_CHANGES_MASTER_FILE = "ultimate_changes_master.json"
ULTIMATE_UNIQORN_LEADERBOARD_MASTER_FILE = "Ultimate_Uniqorn_Leaderboard_Master.xlsx"

SEASON_CHART_FOLDER_MASTER = str(BASE_DIR / "uniqorn-frontend" / "public" / "season-charts-master")
ULTIMATE_CHART_FOLDER_MASTER = str(BASE_DIR / "uniqorn-frontend" / "public" / "ultimate-charts-master")


def run_script(script_name, description, timeout=None):
    """Run a script and measure execution time."""
    print(f"\n{'='*60}")
    print(f"Running: {description}")
    print(f"Script: {script_name}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        kwargs = {"capture_output": True, "text": True}
        if timeout is not None:
            kwargs["timeout"] = timeout
        result = subprocess.run([sys.executable, script_name], **kwargs)
        
        execution_time = time.time() - start_time
        
        if result.returncode == 0:
            print(f"‚úÖ {description} - SUCCESS ({execution_time:.2f}s)")
            if result.stdout:
                print("Output:", result.stdout[-500:])  # Show last 500 chars
        else:
            print(f"‚ùå {description} - FAILED ({execution_time:.2f}s)")
            print("Error:", result.stderr[-500:])  # Show last 500 chars
            
    except subprocess.TimeoutExpired:
        print(f"‚è∞ {description} - TIMEOUT")
    except Exception as e:
        print(f"üí• {description} - ERROR: {e}")


def _parse_bucket_str(bucket_str: str):
    return tuple(int(x) for x in bucket_str.strip("()").split(", "))


def _split_player_name(player: str):
    if not isinstance(player, str) or not player:
        return "Unknown", ""
    if " " not in player:
        return player, ""
    return player.split(" ", 1)


def _ensure_parent_dir(path: str):
    Path(path).parent.mkdir(parents=True, exist_ok=True)


def _write_or_replace_sheet(excel_path: str, sheet_name: str, df: pd.DataFrame):
    from openpyxl import load_workbook

    if Path(excel_path).exists():
        book = load_workbook(excel_path)
        if sheet_name in book.sheetnames:
            del book[sheet_name]
        with pd.ExcelWriter(excel_path, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)
        return

    _ensure_parent_dir(excel_path)
    with pd.ExcelWriter(excel_path, engine="openpyxl", mode="w") as writer:
        df.to_excel(writer, sheet_name=sheet_name, index=False)


def _write_career_top50_sheet(excel_path: str, sheet_name: str = "AllTimeTop20", limit: int = 50):
    from openpyxl import load_workbook

    if not Path(excel_path).exists():
        print(f"‚ö†Ô∏è  {excel_path} not found; skipping career sheet")
        return

    wb = load_workbook(excel_path, read_only=True, data_only=True)
    season_sheets = [s for s in wb.sheetnames if "-" in s]
    if not season_sheets:
        print("‚ö†Ô∏è  No season sheets found in Uniqorn_Master.xlsx; skipping career sheet")
        return

    all_rows = []
    for s in season_sheets:
        try:
            df = pd.read_excel(excel_path, sheet_name=s)
        except Exception:
            continue

        needed = {"personId", "firstName", "lastName", "avg_weighted_uniqueness"}
        if df.empty or not needed.issubset(set(df.columns)):
            continue

        df = df[["personId", "firstName", "lastName", "avg_weighted_uniqueness", "games"]].copy() if "games" in df.columns else df[["personId", "firstName", "lastName", "avg_weighted_uniqueness"]].copy()
        df["avg_weighted_uniqueness"] = pd.to_numeric(df["avg_weighted_uniqueness"], errors="coerce")
        all_rows.append(df)

    if not all_rows:
        print("‚ö†Ô∏è  No valid season data for career sheet; skipping")
        return

    combined = pd.concat(all_rows, ignore_index=True)
    combined = combined.dropna(subset=["avg_weighted_uniqueness", "personId"])

    career = (
        combined.groupby(["personId", "firstName", "lastName"], as_index=False)["avg_weighted_uniqueness"]
        .mean()
        .rename(columns={"avg_weighted_uniqueness": "avg_weighted_uniqueness"})
    )

    career = career.sort_values("avg_weighted_uniqueness", ascending=False).head(limit).reset_index(drop=True)
    _write_or_replace_sheet(excel_path, sheet_name, career)
    print(f"‚úÖ Career Top {limit} written to {excel_path} [{sheet_name}]")


def _render_radar_chart(out_path: str, title: str, values: list[int]):
    points_bins = [0, 5, 10, 15, 20, 25, 30, 40, 50, np.inf]
    assists_bins = [0, 2, 5, 8, 12, 20, np.inf]
    rebounds_bins = [0, 2, 5, 10, 15, 20, np.inf]
    blocks_bins = [0, 1, 3, 5, 7, np.inf]
    steals_bins = [0, 1, 3, 5, 7, np.inf]

    labels = ["PTS", "AST", "REB", "BLK", "STL"]
    max_bucket = max(
        len(points_bins) - 2,
        len(assists_bins) - 2,
        len(rebounds_bins) - 2,
        len(blocks_bins) - 2,
        len(steals_bins) - 2,
    )
    accent = "rgb(56, 189, 248)"

    values_closed = values + [values[0]]
    labels_closed = labels + [labels[0]]

    fig = go.Figure(
        data=[
            go.Scatterpolar(
                r=values_closed,
                theta=labels_closed,
                fill="toself",
                fillcolor="rgba(56, 189, 248, 0.20)",
                line=dict(color=accent, width=3),
                marker=dict(color=accent, size=6),
            )
        ]
    )

    fig.update_layout(
        template="plotly_dark",
        title=dict(text=title, x=0.5, xanchor="center", font=dict(size=14, color="rgba(224, 242, 254, 1)")),
        margin=dict(l=40, r=40, t=70, b=40),
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)",
        polar=dict(
            bgcolor="rgba(0,0,0,0)",
            radialaxis=dict(range=[0, max_bucket], showticklabels=True, tickfont=dict(color="rgba(228,228,231,1)")),
            angularaxis=dict(tickfont=dict(color="rgba(224,242,254,1)")),
        ),
        showlegend=False,
    )

    _ensure_parent_dir(out_path)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    max_retries = 3
    for attempt in range(max_retries):
        try:
            fig.write_image(out_path, width=700, height=700, scale=2)
            return True
        except Exception:
            if attempt == max_retries - 1:
                return False
            time.sleep(1)


def _collect_season_stats(db: MasterBucketDatabase, season: str):
    season_bucket_counts: dict[tuple[int, int, int, int, int], int] = {}
    player_bucket_counts: dict[tuple[int, tuple[int, int, int, int, int]], int] = {}
    season_games: list[tuple[tuple[int, int, int, int, int], dict]] = []
    player_names: dict[int, tuple[str, str]] = {}

    for bucket_str, bucket_info in db.data.items():
        bucket_key = _parse_bucket_str(bucket_str)
        for game in bucket_info.get("games", []):
            if game.get("season") != season:
                continue
            season_games.append((bucket_key, game))
            season_bucket_counts[bucket_key] = season_bucket_counts.get(bucket_key, 0) + 1

            pid = int(game.get("personId", 0))
            player_bucket_counts[(pid, bucket_key)] = player_bucket_counts.get((pid, bucket_key), 0) + 1
            if pid not in player_names:
                fn, ln = _split_player_name(game.get("player", ""))
                player_names[pid] = (fn, ln)

    return season_games, season_bucket_counts, player_bucket_counts, player_names


def generate_master_outputs():
    print("\nüìä Generating Master outputs...")

    db = MasterBucketDatabase()
    season_games, season_bucket_counts, player_bucket_counts, player_names = _collect_season_stats(db, CURRENT_SEASON)

    print("\nüìà Writing Uniqorn leaders (Master)...")
    ALPHA = 0.10
    per_player_sum: dict[int, float] = {}
    per_player_games: dict[int, int] = {}
    for bucket_key, game in season_games:
        pid = int(game.get("personId", 0))
        total = season_bucket_counts.get(bucket_key, 1)
        self_count = player_bucket_counts.get((pid, bucket_key), 0)
        effective_count = max(total - self_count, 1)
        score = float(np.exp(-ALPHA * effective_count))
        per_player_sum[pid] = per_player_sum.get(pid, 0.0) + score
        per_player_games[pid] = per_player_games.get(pid, 0) + 1

    rows = []
    for pid, games in per_player_games.items():
        if games <= 15:
            continue
        fn, ln = player_names.get(pid, ("Unknown", ""))
        rows.append({
            "personId": pid,
            "firstName": fn,
            "lastName": ln,
            "season": CURRENT_SEASON,
            "games": games,
            "avg_weighted_uniqueness": round(per_player_sum.get(pid, 0.0) / games, 4),
        })
    leaders_df = pd.DataFrame(rows)
    if not leaders_df.empty and "avg_weighted_uniqueness" in leaders_df.columns:
        leaders_df = leaders_df.sort_values(by="avg_weighted_uniqueness", ascending=False)
    _write_or_replace_sheet(UNIQORN_LEADERS_MASTER_FILE, CURRENT_SEASON, leaders_df)

    _write_career_top50_sheet(UNIQORN_LEADERS_MASTER_FILE, sheet_name="AllTimeTop20", limit=50)

    print("\nüèÄ Writing most recent games (Master)...")
    if season_games:
        most_recent_date = max(game.get("date") for _, game in season_games)
        recent_games = [(bk, g) for bk, g in season_games if g.get("date") == most_recent_date]
        games_analysis = []
        ALPHA = 0.10
        for bucket_key, game in recent_games:
            pid = int(game.get("personId", 0))
            total = season_bucket_counts.get(bucket_key, 1)
            # Use (total - 1) so that bucket_count=1 gives score 1.0
            # bucket_count=2 gives exp(-0.1*1)=0.9048, etc.
            uniqueness_score = float(np.exp(-ALPHA * (total - 1)))
            games_analysis.append({
                "player": game.get("player"),
                "date": most_recent_date,
                "team": game.get("team"),
                "opponent": game.get("opponent"),
                "season": CURRENT_SEASON,
                "stats": game.get("stats"),
                "bucket_desc": get_bucket_description(bucket_key),
                "bucket_count": int(total),
                "uniqueness_score": round(uniqueness_score, 4),
            })

        games_analysis.sort(key=lambda x: x["uniqueness_score"], reverse=True)
        pd.DataFrame(games_analysis).to_excel(MOST_RECENT_GAMES_MASTER_FILE, index=False)
        print(f"‚úÖ Wrote {len(games_analysis)} rows to {MOST_RECENT_GAMES_MASTER_FILE}")
    else:
        print(f"‚ö†Ô∏è  No season games found for {CURRENT_SEASON}")

    print("\nü¶Ñ Writing current-season Uniqorn games (Master)...")
    uniqorn_season_games = [(bk, g) for bk, g in season_games if season_bucket_counts.get(bk, 0) == 1]
    uniqorn_export_rows = []
    for bucket_key, game in uniqorn_season_games:
        pts, ast, reb, blk, stl = (int(x) for x in str(game.get("stats", "0/0/0/0/0")).split("/"))
        fn, ln = _split_player_name(game.get("player", ""))
        dt = pd.to_datetime(game.get("date"))
        uniqorn_export_rows.append({
            "game_date": dt,
            "firstName": fn,
            "lastName": ln,
            "playerteamName": game.get("team"),
            "opponentteamName": game.get("opponent"),
            "points": pts,
            "assists": ast,
            "rebounds": reb,
            "blocks": blk,
            "steals": stl,
        })

    uniqorn_export_df = pd.DataFrame(uniqorn_export_rows)
    if not uniqorn_export_df.empty and "game_date" in uniqorn_export_df.columns:
        uniqorn_export_df = uniqorn_export_df.sort_values("game_date", ascending=False)
    uniqorn_export_df.to_excel(CURRENT_SEASON_UNIQORN_GAMES_MASTER_FILE, index=False)
    print(f"‚úÖ Wrote {len(uniqorn_export_df)} rows to {CURRENT_SEASON_UNIQORN_GAMES_MASTER_FILE}")

    print("\nüèÜ Writing Ultimate Uniqorns (Master)...")
    ultimate_rows = []

    for bucket_str, bucket_info in db.data.items():
        if int(bucket_info.get("count", 0)) != 1:
            continue
        bucket_key = _parse_bucket_str(bucket_str)
        games = bucket_info.get("games", [])
        if not games:
            continue
        game = games[0]
        pts, ast, reb, blk, stl = (int(x) for x in str(game.get("stats", "0/0/0/0/0")).split("/"))
        fn, ln = _split_player_name(game.get("player", ""))
        dt = pd.to_datetime(game.get("date"))
        ultimate_rows.append({
            "season": game.get("season"),
            "game_date": dt,
            "firstName": fn,
            "lastName": ln,
            "playerteamName": game.get("team"),
            "opponentteamName": game.get("opponent"),
            "points": pts,
            "assists": ast,
            "rebounds": reb,
            "blocks": blk,
            "steals": stl,
        })

    ultimate_df = pd.DataFrame(ultimate_rows)
    if not ultimate_df.empty and "game_date" in ultimate_df.columns:
        ultimate_df = ultimate_df.sort_values("game_date", ascending=False)
    ultimate_df.to_excel(ULTIMATE_UNIQORN_GAMES_MASTER_FILE, index=False)
    print(f"‚úÖ Wrote {len(ultimate_df)} rows to {ULTIMATE_UNIQORN_GAMES_MASTER_FILE}")

    print("\nüîÅ Writing Ultimate changes (Master)...")
    def load_key(df: pd.DataFrame):
        return set(
            (row.firstName, row.lastName, str(pd.to_datetime(row.game_date).date()), int(row.points), int(row.assists), int(row.rebounds), int(row.blocks), int(row.steals))
            for _, row in df.iterrows()
        )

    current_key = load_key(ultimate_df) if not ultimate_df.empty else set()
    prev_key = set()
    if Path(PREVIOUS_ULTIMATE_UNIQORN_GAMES_MASTER_FILE).exists():
        prev_df = pd.read_excel(PREVIOUS_ULTIMATE_UNIQORN_GAMES_MASTER_FILE)
        prev_key = load_key(prev_df)

    new_ultimate = current_key - prev_key
    broken_ultimate = prev_key - current_key

    def to_dict(key_set):
        return [
            {
                "key": f"{t[0]}_{t[1]}_{t[2]}_{t[3]}_{t[4]}_{t[5]}_{t[6]}_{t[7]}",
                "firstName": t[0],
                "lastName": t[1],
                "game_date": t[2],
                "points": t[3],
                "assists": t[4],
                "rebounds": t[5],
                "blocks": t[6],
                "steals": t[7],
            }
            for t in sorted(key_set)
        ]

    changes = {"new": to_dict(new_ultimate), "broken": to_dict(broken_ultimate)}
    with open(ULTIMATE_CHANGES_MASTER_FILE, "w") as f:
        json.dump(changes, f, indent=2)

    ultimate_df.to_excel(PREVIOUS_ULTIMATE_UNIQORN_GAMES_MASTER_FILE, index=False)
    print(f"New: {len(new_ultimate)} | Broken: {len(broken_ultimate)}")

    print("\nüèÖ Writing Ultimate leaderboard (Master)...")
    if not ultimate_df.empty and {"firstName", "lastName"}.issubset(set(ultimate_df.columns)):
        leaderboard = (
            ultimate_df.groupby(["firstName", "lastName"])
            .size()
            .reset_index(name="uniqorn_games")
            .sort_values("uniqorn_games", ascending=False)
            .reset_index(drop=True)
        )
        leaderboard.to_excel(ULTIMATE_UNIQORN_LEADERBOARD_MASTER_FILE, index=False)
        print(f"‚úÖ Wrote {len(leaderboard)} rows to {ULTIMATE_UNIQORN_LEADERBOARD_MASTER_FILE}")
    else:
        print("‚ö†Ô∏è  Ultimate master dataframe empty")

def main():
    print("üöÄ Starting FAST Daily Pipeline (Master Bucket System)")
    print("üì° Data Source: NBA API (Official)")
    print(f"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Check if master database exists
    try:
        db = MasterBucketDatabase()
        stats = db.get_statistics()
        if not stats.get("total_games"):
            print("‚ùå Master database is missing or empty. Run: python master_bucket_precompute.py")
            return
        print(f"üìÇ Master Database: {stats['total_games']:,} games, {stats['total_buckets']:,} buckets")
    except Exception as e:
        print(f"‚ùå Master database not found! Run master_bucket_precompute.py first: {e}")
        return
    
    # Pipeline steps - now using NBA API instead of Kaggle
    # Removed: ("incremental_update.py", "Incremental data update (Kaggle -> PlayerStatistics.csv)", None)
    # The incremental_update_new.py now fetches directly from NBA API via data_utils
    pipeline_steps = [
        ("incremental_update_new.py", "Incremental update from NBA API (Master DB)", None),
        ("generate_master_outputs", "Generate Master outputs", None),
    ]
    
    total_start_time = time.time()
    successful_steps = 0
    
    for script_name, description, timeout in pipeline_steps:
        if script_name == "generate_master_outputs":
            generate_master_outputs()
            successful_steps += 1
        else:
            run_script(script_name, description, timeout=timeout)
            successful_steps += 1
        
        # Brief pause between steps
        time.sleep(1)
    
    total_time = time.time() - total_start_time
    
    # Show final statistics
    print(f"\n{'='*60}")
    print("üìä FINAL STATISTICS")
    print(f"{'='*60}")
    
    try:
        db = MasterBucketDatabase()
        stats = db.get_statistics()
        print(f"üèÄ Total Games: {stats['total_games']:,}")
        print(f"üî¢ Total Buckets: {stats['total_buckets']:,}")
        print(f"ü¶Ñ Uniqorn Games: {stats['uniqorn_count']:,} ({stats['uniqorn_percentage']:.2f}%)")
        print(f"üë• Two-Occurrence Games: {stats['two_occurrence_count']:,}")
        print(f"üìÖ Date Range: {stats['date_range']['start']} to {stats['date_range']['end']}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not load final statistics: {e}")
    
    print(f"\n‚úÖ Successful steps: {successful_steps}/{len(pipeline_steps)}")
    print(f"‚è±Ô∏è  Total execution time: {total_time:.2f}s")
    print(f"üèÅ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if successful_steps == len(pipeline_steps):
        print("üéâ Fast pipeline completed successfully!")
        print("üí° Performance improvement: 10-100x faster than original pipeline")
    else:
        print("‚ö†Ô∏è  Some pipeline steps failed. Check logs above.")

if __name__ == "__main__":
    main()
